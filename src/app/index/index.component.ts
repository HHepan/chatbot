import {AfterViewChecked, Component, OnInit, ViewChild} from '@angular/core';
import {XunFeiApiService} from "../../services/xunfei-api.service";
import {Message} from "../../../app/entity/message";
import {DatePipe} from "@angular/common";
import {IndexService} from "../../services/index.service";

@Component({
  selector: 'app-index',
  templateUrl: './index.component.html',
  styleUrls: ['./index.component.scss']
})
export class IndexComponent implements AfterViewChecked, OnInit {
  @ViewChild('chatBody') private chatBody: any;
  messageContent: string = '';
  messages: Message[] = [];
  mode: 'text' | 'audio' = 'text'; // Default to 'text' mode
  recording = false;
  stream: MediaStream | null = null;
  audioContext: AudioContext | null = null;
  analyser: AnalyserNode | null = null;
  dataArray: Uint8Array | null = null;
  canvas: HTMLCanvasElement | null = null;
  canvasContext: CanvasRenderingContext2D | null = null;

  speechRecognitionText = '';
  naturalLanguageResult = '';

  userRole = {
    robot: 0,
    user: 1
  }

  constructor(private xunFeiApiService: XunFeiApiService,
              private datePipe: DatePipe,
              private indexService: IndexService) {
  }

  ngOnInit(): void {
    this.getAllMessage();
  }

  getAllMessage() {
    this.indexService.getAll().subscribe(allMessages => {
      // console.log('getAllMessage', allMessages);
      allMessages.forEach((message: Message) => {
        this.messages.push(message);
      });
    });
  }

  sendMessage() {
    this.naturalLanguageResult = '';
    if (this.messageContent.trim()) {
      this.addMessage(this.messageContent, this.userRole.user);

      const naturalLanguageApiObserver = this.xunFeiApiService.naturalLanguageApi(this.messageContent);

      // this.messages.push({ text: this.message, sender: 'user' });
      this.messageContent = '';

      const subscription = naturalLanguageApiObserver.subscribe(result => {
        if (result === '[DONE]') {
          subscription.unsubscribe(); // åœæ­¢æ¥æ”¶åç»­æ•°æ®
          console.log('å·²å®Œæˆå“åº”ï¼Œå–æ¶ˆè®¢é˜…');
          return;
        }

        this.naturalLanguageResult += result;
        console.log('è‡ªç„¶è¯­è¨€å¤„ç†ç»“æœè¿”å›Cå±‚ï¼š', this.naturalLanguageResult);
      });

      // æ¨¡æ‹Ÿæœºå™¨äººçš„å“åº”
      // setTimeout(() => {
      //   this.messages.push({ text: 'Thank you for your message!', sender: 'bot' });
      //   this.scrollToBottom();
      // }, 1000);
    }
  }

  addMessage(messageContent: string, role: number) {
    const message = new Message();

    message.content = messageContent;
    message.time = this.getFormattedTime();
    message.role = role;

    // console.log('addMessage', message);
    this.indexService.add(message).subscribe(result => {
      const newestMessage = result[result.length - 1];
      console.log('add message success', newestMessage);
      // this.messages.push(newestMessage);
      this.getAllMessage();
    })
  }

  getFormattedTime(): string {
    const now = new Date();
    return this.datePipe.transform(now, 'yyyy-MM-dd HH:mm:ss') || '';
  }

  scrollToBottom() {
    try {
      this.chatBody.nativeElement.scrollTop = this.chatBody.nativeElement.scrollHeight;
    } catch (err) {
      console.error('Error scrolling to bottom:', err);
    }
  }

  // åœ¨è§†å›¾æ›´æ–°åè°ƒç”¨æ»šåŠ¨
  ngAfterViewChecked() {
    this.scrollToBottom();
  }

  // ä¿®æ”¹æ¨¡å¼ï¼šæ–‡æœ¬/è¯­éŸ³
  toggleMode(mode: 'text' | 'audio') {
    this.mode = mode;
    if (mode === 'text') {
      this.stopMic();
    }
  }

  // å¼€å¯éº¦å…‹é£
  startMic() {
    navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
      this.stream = stream;
      this.recording = true;

      this.audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
      this.analyser = this.audioContext.createAnalyser();
      this.analyser.fftSize = 256;  // Choose FFT size for frequency analysis
      this.dataArray = new Uint8Array(this.analyser.frequencyBinCount);

      const source = this.audioContext.createMediaStreamSource(stream);
      source.connect(this.analyser);

      this.canvas = document.getElementById('waveform') as HTMLCanvasElement;
      this.canvasContext = this.canvas.getContext('2d')!;

      this.drawWaveform();

      console.log('ğŸ™ï¸ éº¦å…‹é£å·²å¯åŠ¨', stream);

      const SAMPLE_RATE = 16000; // ç›®æ ‡é‡‡æ ·ç‡16K
      const processor = this.audioContext.createScriptProcessor(4096, 1, 1);
      source.connect(processor);
      processor.connect(this.audioContext.destination);

      processor.onaudioprocess = (event) => {
        if (!this.recording) return;

        const input = event.inputBuffer.getChannelData(0); // 32-bit float [-1.0, 1.0]
        const downSampled = this.downSampleBuffer(input, this.audioContext!.sampleRate, SAMPLE_RATE);
        const pcm = this.floatTo16BitPCM(downSampled);

        // è°ƒç”¨è¯­éŸ³è¯†åˆ«
        this.xunFeiApiService.speechRecognitionApi(pcm).subscribe(text => {
          this.speechRecognitionText = text;
          console.log('è¯­éŸ³è¯†åˆ«ç»“æœä¼ å›åˆ°Cå±‚ï¼š', text);
        });
      };
    }).catch(err => {
      console.error('ğŸš« æ— æ³•è®¿é—®éº¦å…‹é£:', err);
    });
  }

  // å…³é—­éº¦å…‹é£
  stopMic() {
    this.stream?.getTracks().forEach(track => track.stop());
    this.recording = false;
    console.log('ğŸ›‘ éº¦å…‹é£å·²å…³é—­');

    if (this.audioContext && this.audioContext?.state !== 'closed') {
      this.audioContext.close();
    }
    this.xunFeiApiService.stopSpeechRecognition();
  }

  // ç”»å‡ºè¯­éŸ³æ—¶çš„æ³¢å½¢å›¾
  drawWaveform() {
    if (!this.analyser || !this.canvasContext || !this.dataArray) return;

    this.canvasContext.clearRect(0, 0, this.canvas!.width, this.canvas!.height);

    this.analyser.getByteTimeDomainData(this.dataArray);

    this.canvasContext.beginPath();
    const width = this.canvas!.width;
    const height = this.canvas!.height;
    const sliceWidth = width / this.dataArray.length;
    let x = 0;

    for (let i = 0; i < this.dataArray.length; i++) {
      const v = this.dataArray[i] / 128.0; // Normalize to [0, 1]
      const y = v * height / 2; // Scale to canvas height
      if (i === 0) {
        this.canvasContext.moveTo(x, y);
      } else {
        this.canvasContext.lineTo(x, y);
      }
      x += sliceWidth;
    }

    this.canvasContext.lineTo(this.canvas!.width, this.canvas!.height / 2);
    this.canvasContext.strokeStyle = '#56C596';
    this.canvasContext.lineWidth = 2;
    this.canvasContext.stroke();

    if (this.recording) {
      requestAnimationFrame(() => this.drawWaveform());
    }
  }

  // è½¬æ¢å½•éŸ³æ–‡ä»¶çš„æ ¼å¼
  downSampleBuffer(buffer: Float32Array, inputSampleRate: number, outputSampleRate: number): Float32Array {
    if (outputSampleRate === inputSampleRate) return buffer;
    const ratio = inputSampleRate / outputSampleRate;
    const length = Math.round(buffer.length / ratio);
    const result = new Float32Array(length);
    let offset = 0;
    for (let i = 0; i < result.length; i++) {
      const nextOffset = Math.round((i + 1) * ratio);
      let sum = 0;
      let count = 0;
      for (let j = offset; j < nextOffset && j < buffer.length; j++) {
        sum += buffer[j];
        count++;
      }
      result[i] = sum / count;
      offset = nextOffset;
    }
    return result;
  }

  // è½¬æ¢å½•éŸ³æ–‡ä»¶çš„æ ¼å¼ä¸ºpcm
  floatTo16BitPCM(input: Float32Array): Int16Array {
    const output = new Int16Array(input.length);
    for (let i = 0; i < input.length; i++) {
      const s = Math.max(-1, Math.min(1, input[i]));
      output[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
    }
    return output;
  }
}
